# MLLM_PCD â€“ Multimodal Large Language Model for Video Question Answering

This project focuses on developing a **multimodal language model** for **video-based question answering** using advanced deep learning techniques. The model is designed to process both video and text inputs to answer questions based on movie clips, leveraging recent advances in cross-modal learning.

## Models and Components

Three major components were integrated into the model pipeline:

- **Video Encoder** (CLIP ViT-L/14):
  - Extracts frame-level visual features from video clips.
  
- **Cross-Attention Layer**:
  - Aligns and fuses information between the video and text modalities.

- **Mixture of Experts (MoE)**:
  - Uses 4 experts to specialize the representation learning and improve performance.

- **Vicuna-v1.1 (LLM)**:
  - Processes the fused embeddings and generates natural language answers.

## Dataset

The **MTVQA (Movie Text-Video Question Answering)** dataset was used for training and evaluation. It contains:

- Short video clips from movies  
- Natural language questions and answers  
- Optional subtitles and metadata  
- Multiple-choice answer format

Video frames were extracted, and both visual and textual information were preprocessed for multimodal fusion.

## âš™ï¸ Tech Stack

### ğŸ–¥ï¸ Frontend

- **React 19**: Latest React version for building the UI  
- **Vite**: Fast build tooling and development server  
- **Zustand**: Simple state management for React  
- **React Icons**: Icon library for UI elements  
- **CSS3**: Custom styling with CSS variables for theming  


## Technologies Used

- **Python 3.10**: Core programming language.
- **PyTorch 1.13.1 (CUDA 11.7)**: For deep learning model implementation and training.
- **Hugging Face Transformers**: For model loading, tokenization, and LLM integration.
- **CLIP ViT-L/14**: Video encoder for feature extraction.
- **Vicuna-v1.1**: Language model for generating answers.
- **Google Colab (T4 GPU)**: For training and experimentation.

## ğŸ“ Project Structure
PCD-main/
â”œâ”€â”€ videochat/ # Frontend application
â”‚ â”œâ”€â”€ public/ # Static assets
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ assets/ # Application assets
â”‚ â”‚ â”‚ â”œâ”€â”€ stores/ # Zustand store definitions
â”‚ â”‚ â”‚ â””â”€â”€ react.svg # React logo
â”‚ â”‚ â”œâ”€â”€ components/ # React components
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatInterface.jsx # Main chat component
â”‚ â”‚ â”‚ â””â”€â”€ ChatInterface.css # Styling for chat component
â”‚ â”‚ â”œâ”€â”€ App.jsx # Main application component
â”‚ â”‚ â”œâ”€â”€ App.css # Application styles
â”‚ â”‚ â”œâ”€â”€ main.jsx # Application entry point
â”‚ â”‚ â””â”€â”€ index.css # Global styles
â”‚ â”œâ”€â”€ .gitignore # Git ignore file
â”‚ â”œâ”€â”€ package.json # Project dependencies
â”‚ â”œâ”€â”€ eslint.config.js # ESLint configuration
â”‚ â”œâ”€â”€ vite.config.js # Vite configuration
â”‚ â””â”€â”€ README.md # This file


## ğŸš€ Getting Started

### Prerequisites

- **Node.js 18+**
- **npm** or **yarn**


## Preliminary Results

*(Training still in progress)* â€“ Below are expected or early-stage performance metrics on a small subset of MTVQA:

- **Model (Cross-Attention + MoE + Vicuna)**:
  - Accuracy: *to be determined*
  - BLEU / METEOR: *to be computed*
  - Qualitative outputs show improved reasoning over vision-only or text-only baselines.

## Acknowledgements

- Video QA dataset sourced from the [MTVQA GitHub repository](https://github.com/jayleicn/mTVRetrieval).
- LLM and CLIP components inspired by LLaVA and Flamingo architectures.
- Based on research in multimodal machine learning and MLLMs.
